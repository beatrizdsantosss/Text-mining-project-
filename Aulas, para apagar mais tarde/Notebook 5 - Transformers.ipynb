{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be00da1",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a47b8",
   "metadata": {},
   "source": [
    "This notebook focuses on the widely known Hugging Face library to apply different types of transformer models in a distinct range of tasks.\n",
    "\n",
    "We hope you learn how you can leverage pretrained transformer-based models and how to fine-tune them to a specific downstream task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf0a3ca",
   "metadata": {},
   "source": [
    "To dive deep into transformers, we recommend you to start by reading\n",
    "    \n",
    "- http://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "This gives a step by step explanation of the original paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "![4](https://lilianweng.github.io/lil-log/assets/images/transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f3541",
   "metadata": {},
   "source": [
    "#### Code implementation\n",
    "\n",
    "It's also good pratice to try implementing yourself the original code before using a Transformers library:\n",
    "- http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cca0df",
   "metadata": {},
   "source": [
    "# Hugging Face ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500226a2",
   "metadata": {},
   "source": [
    "\"Training a transformer model and deploying these models can be quite challeging. In general these models have millions to tens of billions of parameters and requires large amount of data. \n",
    "\n",
    "This becomes very costly in terms of time and compute resources. It even translates to environmental impact. Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!\n",
    "\n",
    "\n",
    "The Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. \n",
    "\n",
    "The Hugging Face Transformers library provides the functionality to create and use those shared models.\"\n",
    "\n",
    "For more details look at the official [Hugging Face course](https://huggingface.co/course/chapter1/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dd3962",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93504fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229ff515d0d14686b4a5ea68e31de87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bc944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b20bbc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install transformers\n",
    "#!pip3 install ipywidgets --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf1f14",
   "metadata": {},
   "source": [
    "### Important:\n",
    "\n",
    "After this instalation --> don't forget to restart the kernel of the jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a61b8b",
   "metadata": {},
   "source": [
    "## Examples of transformer architectures available\n",
    "\n",
    "As menioned above, the Transformer architecture was introduced in the paper [Attention Is All You Need](https://arxiv.org/abs/1706.03762), 2017, in which the focus of the original research was on translation tasks using encoder-decoder blocks. \n",
    "\n",
    "This was then followed by the introduction of several influential models, including encoder-only models (e.g., BERT) and decoder-only models (e.g., GPT), and there have been also a variety of encoder-decoder transformer based models (e.g., BART and T5). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9c4f91",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a572ab73",
   "metadata": {},
   "source": [
    "Encoder models use only the encoder block of a Transformer model. \n",
    "\n",
    "These models are often characterized as having ‚Äúbi-directional‚Äù attention, and are often called auto-encoding models.\n",
    "\n",
    "Encoder models are best suited for tasks requiring an understanding of the full sentence, such as:\n",
    "   - sentence classification\n",
    "   - named entity recognition (word classification in general), \n",
    "   - extractive question answering\n",
    "   - sentence representation (contextual embeddings)\n",
    "\n",
    "    \n",
    "The pretraining of these models usually revolves around somehow corrupting a given sentence (for instance, by masking random words in it) and tasking the model with finding or reconstructing the initial sentence.\n",
    "\n",
    "\n",
    "There are a variety of encoder models available at Hugging Face. Some examples include:\n",
    "\n",
    "- [BERT](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "- [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert)\n",
    "- [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569c62a",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "Decoder models use only the decoder of a Transformer model. At each stage, for a given word, the self-attention layers can only access the words positioned before it in the sentence. These models are often called auto-regressive models.\n",
    "\n",
    "The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
    "\n",
    "These models are best suited for tasks involving text generation.\n",
    "\n",
    "Some examples of decoder models available in Hugging Face include:\n",
    "- [GPT](https://huggingface.co/docs/transformers/model_doc/openai-gpt)\n",
    "- [GPT-2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n",
    "- [Transformer XL](https://huggingface.co/docs/transformers/model_doc/transfo-xl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b257cd",
   "metadata": {},
   "source": [
    "## Encoder-decoder models\n",
    "\n",
    "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture. \n",
    "\n",
    "Sequence-to-sequence models are best suited for tasks revolving around generating new sentences depending on a given input (conditional text generation), such as:\n",
    "- summarization\n",
    "- translation\n",
    "- or generative question answering\n",
    "\n",
    "Representatives of this family of models include:\n",
    "\n",
    "- [BART](https://huggingface.co/docs/transformers/model_doc/bart)\n",
    "- [Marian](https://huggingface.co/docs/transformers/model_doc/marian)\n",
    "- [T5](https://huggingface.co/docs/transformers/model_doc/t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea21065",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0df12a",
   "metadata": {},
   "source": [
    "The most basic object in the Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n",
    "\n",
    "See all details of pipeline here: https://huggingface.co/docs/transformers/v4.16.0/en/main_classes/pipelines#transformers.TranslationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0a1da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\migue\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\utils\\generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\"I've been waiting for a HuggingFace course my whole life.\",\n",
    "            \"I hate this so much!\"])\n",
    "\n",
    "# By default, this pipeline selects a particular pretrained model \n",
    "# that has been fine-tuned for sentiment analysis in English. \n",
    "\n",
    "# The model is downloaded and cached when you create the classifier object. \n",
    "# If you rerun the command, the cached model will be used instead \n",
    "# and there is no need to download the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093a9bd",
   "metadata": {},
   "source": [
    "Try with your own text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9c90a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " The restaurant definitely could be better\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.974227249622345}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = input()\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892547fc",
   "metadata": {},
   "source": [
    "Besides sentiment analysis, some of the currently available pipelines are:\n",
    "\n",
    "- feature-extraction (get the vector representation of a text)\n",
    "- ner (named entity recognition)\n",
    "- question-answering\n",
    "- summarization\n",
    "- text-generation\n",
    "- translation\n",
    "- zero-shot-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c3caf3",
   "metadata": {},
   "source": [
    "Let‚Äôs now for instance check how to use a pipeline to generate some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d20b71e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use transformer models and construct your own transformers. We will see that you can create or modify a transformer transformer in a given class and call it transformers. You will be able to build your own'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to use transformer models and\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197e4365",
   "metadata": {},
   "source": [
    "The previous examples used the default model for the task at hand, but you can also choose a particular model for any of the above tasks. \n",
    "\n",
    "For the specific case of the generation task, you can also control:\n",
    "- how many different sequences are generated with the argument num_return_sequences\n",
    "- the total length of the output text with the argument max_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c500502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9278b1be78d14f3aa84ede78c829ad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1048c78dc94f3f96b81204dc3c0f34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8ea8f6da854653b1a46ad16d354b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "124337118b644d60aa1406426b0ad438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb433c15a7e43ac801c6ac5f88e7070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6988bc295b824e20bead96c96dcd6cf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4ba36293544124b45ecd96c8bf3110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I was waiting for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I was waiting for the driver to get off the bike in one of the few doors of a shop where his girlfriend was working. Now he has the'},\n",
       " {'generated_text': 'I was waiting for me again on campus and did nothing to go back but watch that he got knocked unconscious. I took my leave and looked to be'},\n",
       " {'generated_text': 'I was waiting for the most dramatic moment in a huge warzone in which the United States was at war. The United States was the first non-'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    input(),\n",
    "    max_length=30,\n",
    "    num_return_sequences=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5721a",
   "metadata": {},
   "source": [
    "Try it yourself with different pipelines available. \n",
    "\n",
    "To check how to use one in specific see the documentation: https://huggingface.co/docs/transformers/v4.16.0/en/main_classes/pipelines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7951592",
   "metadata": {},
   "source": [
    "\n",
    "# Pipeline with a real dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ce5e3",
   "metadata": {},
   "source": [
    "Let's try this pipeline with sentences from a dataset, such as the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f6efe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "from torchtext.data import Field, LabelField\n",
    "\n",
    "TEXT = data.Field()\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "_, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae980a23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ideally we should run all the sentences to the model and see it's performance on test (or validation set).\n",
    "# But lets just pick first some number of sentences to run faster (or you could use GPU): such as first 50 sentences and last 50 sentences\n",
    "labels=list(test_data.label)[:50]+list(test_data.label)[-50:] \n",
    "sents_with_tokens=list(test_data.text)[:50]+list(test_data.text)[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecce6dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We need to have the corresponding sentences (and not tokens) \n",
    "# so that the pipeline tokenizes the text by itself according to the model tokenization\n",
    "# we can give a max n¬∫ of words to be faster (e.g., in terms of performing self attention)\n",
    "MAX_LEN=500\n",
    "sents=[\" \".join(tokens)[:MAX_LEN] for tokens in sents_with_tokens] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5510b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:07<00:00, 12.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "preds=[]\n",
    "for sent in tqdm(sents):\n",
    "    preds.append(classifier(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10f81e86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc 0.85\n"
     ]
    }
   ],
   "source": [
    "count_correct=0\n",
    "for i in range(len(preds)):\n",
    "    if labels[i] in preds[i][0].get(\"label\").lower():\n",
    "        count_correct +=1\n",
    "print(\"acc\", count_correct/len(labels))              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ecaaa",
   "metadata": {},
   "source": [
    "# Dive deep into Hugging Face\n",
    "\n",
    "\n",
    "Besides the currently available pipelines, we can dive deep and use any model available and apply it to any given task. \n",
    "\n",
    "We‚Äôll dive into the model and configuration classes, show you how to load a model and how it processes numerical inputs to output predictions. \n",
    "\n",
    "\n",
    "## Behind pipeline \n",
    "Let's begin with an end-to-end example where we use a model and a tokenizer together to replicate the pipeline() function of sentiment analysis introduced before.\n",
    "\n",
    "There are three main steps involved when you pass some text to a pipeline:\n",
    "1. <b> Preprocessing with a tokenizer:</b> The text is preprocessed into a format the model can understand.\n",
    "2. <b> Going through the model:</b> The preprocessed inputs are passed to the model.\n",
    "3. <b> Postprocessing the output\n",
    ": </b> The predictions of the model are post-processed, so you can make sense of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d43b8",
   "metadata": {},
   "source": [
    "![0](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a5578",
   "metadata": {},
   "source": [
    "### 1. Preprocessing with a tokenizer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb95681",
   "metadata": {},
   "source": [
    "Like other neural networks, Transformer models can‚Äôt process raw text directly.\n",
    "\n",
    "So the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. \n",
    "\n",
    "To do this we use a <b> tokenizer</b>. They serve the purpose to translate text into data that can be processed by the model.\n",
    "\n",
    "Tokenizer will be responsible for:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "- Adding additional inputs that may be useful to the model (e.g., attention mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc3386",
   "metadata": {},
   "source": [
    "#### Define tokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da41091",
   "metadata": {},
   "source": [
    "The tokenizer and the model should always be from the same checkpoint. Therefore, we need to define the tokenizer with the checkpoint name of the corresponding model. \n",
    "\n",
    "\n",
    "To do this, we use the <b> AutoTokenizer class </b> and its <b> from_pretrained() </b> method with the checkpoint name of the corresponding model inside it.\n",
    "\n",
    "This will automatically fetch the data associated with the model‚Äôs tokenizer (as the vocabulary) and cache it (so it‚Äôs only downloaded the first time you run the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac1e1517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d246f9e5d34f98b8a7fa78457c0ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0583fde881e46a1bc5e9cd24ac902c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ea9e72154b4734becb3c849cab4f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258b3ba85a364c24b53fa77c39d974fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a907a3",
   "metadata": {},
   "source": [
    "Alternatively to the wrapper AutoModel class, you can use directly the class of the corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c6b148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa95f75e",
   "metadata": {},
   "source": [
    "You can use either one of the two. But note that the AutoTokenizer produces checkpoint-agnostic code\n",
    "\n",
    "- The AutoTokenizer will work for other checkpoints besides BERT (e.g.: distilbert-base-uncased-finetuned-sst-2-english), \n",
    "- Whereas the BertTokenizer just works for checkpoints related to BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d39d75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612b130e",
   "metadata": {},
   "source": [
    "To mimic our sentiment analysis pipeline, let's actually use the DistilBERT model and thus use the corresponding DistilBERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d6434cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650b1e6",
   "metadata": {},
   "source": [
    "#### Encode the sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0aa0f4",
   "metadata": {},
   "source": [
    "Once we have the tokenizer, we can directly pass our sentences to it.\n",
    "\n",
    "Translating text to numbers is known as encoding. Encoding is done in a two-step process: \n",
    "- the tokenization\n",
    "- followed by the conversion to input IDs.\n",
    "\n",
    "---------\n",
    "To know more about each of the 2 steps:\n",
    "The first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
    "\n",
    "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dbfd46cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  2424,  2023,  4297, 25377,  2890, 10222, 19307,   999,\n",
      "           102,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"I find this incomprehensible!\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    raw_inputs,\n",
    "    padding=True,        # since the sentences might not have the same size, don't forget to padding. \n",
    "    return_tensors=\"pt\"  # to return with pytorch tensors\n",
    ")\n",
    "\n",
    "# - Feeding your raw_sentences to the tokenizer will give the corresponding input_id\n",
    "# - There is also an attention_mask, useful so that there is no self attention between padding words. \n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1443ce34",
   "metadata": {},
   "source": [
    "Note that each word can correspond to more than one id, since DistilBERT uses subword tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f48799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'find', 'this', 'inc', '##omp', '##re', '##hen', '##sible', '!']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(raw_inputs[2])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd95557",
   "metadata": {},
   "source": [
    "Besides encoding the corresponding input text, we can also decode, going the other way around: from vocabulary indices to get the corresponding string.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3dd74d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens id: [101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "\n",
      "Decoded inputs: [CLS] i've been waiting for a huggingface course my whole life. [SEP]\n",
      "\n",
      "Without special tokens: i've been waiting for a huggingface course my whole life.\n"
     ]
    }
   ],
   "source": [
    "tokens_id = tokenizer.encode(raw_inputs[0])\n",
    "print(\"Tokens id:\", tokens_id)\n",
    "\n",
    "decode_inputs = tokenizer.decode(tokens_id)\n",
    "print(\"\\nDecoded inputs:\", decode_inputs)\n",
    "\n",
    "decode_inputs = tokenizer.decode(tokens_id, skip_special_tokens=True)\n",
    "print(\"\\nWithout special tokens:\", decode_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f5fef",
   "metadata": {},
   "source": [
    "# Going through the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f5fd63",
   "metadata": {},
   "source": [
    "#### Define model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56d2374",
   "metadata": {},
   "source": [
    "We can download our model that is already trained in the same way that we did with our tokenizer:\n",
    "\n",
    "- You can use AutoModel class which also has from_pretrained() method or use directly\n",
    "- You could replace AutoModel directly with the corresponding model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3fc9ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# we have downloaded the same checkpoint we used in our pipeline before\n",
    "# threfore it should actually have been cached already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7e677b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel\n",
    "\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76348c8e",
   "metadata": {},
   "source": [
    "It's also possible to load a model from scratch without trained weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "406486b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "config = DistilBertConfig()\n",
    "model_without_pretrained = DistilBertModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad3906d-2306-4654-984b-0a3e960a0633",
   "metadata": {},
   "source": [
    "#### Feed the inputs\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "We can now feed the inputs we preprocessed before (with the tokenizer) to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a22b0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
      "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
      "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
      "         ...,\n",
      "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
      "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
      "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
      "\n",
      "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
      "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
      "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
      "         ...,\n",
      "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
      "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
      "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]],\n",
      "\n",
      "        [[-0.7638,  0.6725,  0.0017,  ..., -0.1090, -0.3703,  0.1758],\n",
      "         [-0.6105,  0.9168, -0.1487,  ..., -0.1759, -0.0915,  0.1980],\n",
      "         [-1.0326,  0.9284,  0.1013,  ..., -0.3298, -0.1823,  0.2217],\n",
      "         ...,\n",
      "         [-0.7115,  0.8032,  0.1921,  ..., -0.0418, -0.3732,  0.1158],\n",
      "         [-0.7063,  0.7189,  0.1074,  ..., -0.0497, -0.4204,  0.1890],\n",
      "         [-0.6971,  0.7245,  0.1184,  ..., -0.0518, -0.3984,  0.1707]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(**inputs) # you can give further arguments: output_attentions=True and output_hidden_states:True\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfac4c",
   "metadata": {},
   "source": [
    "Note that the outputs of Transformers models behave like named tuples or dictionaries. \n",
    "\n",
    "You can access the elements by:\n",
    "- attributes (like we did) \n",
    "- or by key (outputs[\"last_hidden_state\"])\n",
    "- or even by index if you know exactly where the thing you are looking for is (outputs[0]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4cf25d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state tensor([[[-0.1798,  0.2333,  0.6321,  ..., -0.3017,  0.5008,  0.1481],\n",
      "         [ 0.2758,  0.6497,  0.3200,  ..., -0.0760,  0.5136,  0.1329],\n",
      "         [ 0.9046,  0.0985,  0.2950,  ...,  0.3352, -0.1407, -0.6464],\n",
      "         ...,\n",
      "         [ 0.1466,  0.5661,  0.3235,  ..., -0.3376,  0.5100, -0.0561],\n",
      "         [ 0.7500,  0.0487,  0.1738,  ...,  0.4684,  0.0030, -0.6084],\n",
      "         [ 0.0519,  0.3729,  0.5223,  ...,  0.3584,  0.6500, -0.3883]],\n",
      "\n",
      "        [[-0.2937,  0.7283, -0.1497,  ..., -0.1187, -1.0227, -0.0422],\n",
      "         [-0.2206,  0.9384, -0.0951,  ..., -0.3643, -0.6605,  0.2407],\n",
      "         [-0.1536,  0.8988, -0.0728,  ..., -0.2189, -0.8528,  0.0710],\n",
      "         ...,\n",
      "         [-0.3017,  0.9002, -0.0200,  ..., -0.1082, -0.8412, -0.0861],\n",
      "         [-0.3338,  0.9674, -0.0729,  ..., -0.1952, -0.8181, -0.0634],\n",
      "         [-0.3454,  0.8824, -0.0426,  ..., -0.0993, -0.8329, -0.1065]],\n",
      "\n",
      "        [[-0.7638,  0.6725,  0.0017,  ..., -0.1090, -0.3703,  0.1758],\n",
      "         [-0.6105,  0.9168, -0.1487,  ..., -0.1759, -0.0915,  0.1980],\n",
      "         [-1.0326,  0.9284,  0.1013,  ..., -0.3298, -0.1823,  0.2217],\n",
      "         ...,\n",
      "         [-0.7115,  0.8032,  0.1921,  ..., -0.0418, -0.3732,  0.1158],\n",
      "         [-0.7063,  0.7189,  0.1074,  ..., -0.0497, -0.4204,  0.1890],\n",
      "         [-0.6971,  0.7245,  0.1184,  ..., -0.0518, -0.3984,  0.1707]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"last hidden state\", outputs.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e3f47",
   "metadata": {},
   "source": [
    "Note also that given the input to the model, it will output a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
    "\n",
    "The vector output by the Transformer module generally has three dimensions:\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (3 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c2dca71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size torch.Size([3, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"size\", outputs.last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e373d",
   "metadata": {},
   "source": [
    "## Postprocessing the output\n",
    " \n",
    "While the hidden states that were outputed can be useful on their own, in this case (sentiment analysis), we are more interested in using the corresponding classification head. \n",
    "\n",
    "For our example, we will use the DistilBERT model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won‚Äôt actually use the DistilBertModel class but the DistilBertForSequenceClassification (or AutoModelForSequenceClassification class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f0060cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "    \"I find this incomprehensible!\"\n",
    "]\n",
    "inputs = tokenizer(\n",
    "    raw_inputs,\n",
    "    padding=True,        # since the sentences might not have the same size, don't forget to padding. \n",
    "    return_tensors=\"pt\"  # to return with pytorch tensors\n",
    ")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e866f254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5607,  1.6123],\n",
       "        [ 4.1692, -3.3464],\n",
       "        [ 3.7679, -3.0282]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25fbc54",
   "metadata": {},
   "source": [
    "Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. \n",
    "\n",
    "To be converted to probabilities, they need to go through a SoftMax layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "01e83cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04],\n",
      "        [9.9888e-01, 1.1169e-03]])\n"
     ]
    }
   ],
   "source": [
    "predictions = torch.nn.functional.softmax(logits, dim=-1) # A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2a945a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been waiting for a HuggingFace course my whole life. POSITIVE\n",
      "I hate this so much! NEGATIVE\n",
      "I find this incomprehensible! NEGATIVE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_inputs\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i][1].item()>=0.5:\n",
    "        print(raw_inputs[i],model.config.id2label[1])\n",
    "    else:\n",
    "        print(raw_inputs[i],model.config.id2label[0])\n",
    "\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477b3253",
   "metadata": {},
   "source": [
    "We have successfully reproduced the three steps of the pipeline: preprocessing with tokenizers, passing the inputs through the model, and postprocessing!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0dd54",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers for Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ebeaec6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "#! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6253a0c",
   "metadata": {},
   "source": [
    "Translation converts a sequence of text from one language to another. It is one of several tasks you can formulate as a sequence-to-sequence problem, a powerful framework for returning some output from an input, like translation or summarization. Translation systems are commonly used for translation between different language texts, but it can also be used for speech or some combination in between like text-to-speech or speech-to-text.\n",
    "\n",
    "![3](https://raw.githubusercontent.com/huggingface/notebooks/c5d94e54a771af91c6732a5313c49c2c42ac5cff/examples/images/translation.png)\n",
    "\n",
    "This guide will show you how to:\n",
    "\n",
    "1. Finetune [T5](https://huggingface.co/t5-small) on the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset to translate English text to French.\n",
    "2. Use your finetuned model for inference.\n",
    "\n",
    "<Tip>\n",
    "The task illustrated in this tutorial is supported by the following model architectures:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [Blenderbot](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot), [BlenderbotSmall](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/blenderbot-small), [Encoder decoder](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/encoder-decoder), [FairSeq Machine-Translation](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fsmt), [GPTSAN-japanese](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptsan-japanese), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LongT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longt5), [M2M100](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/m2m_100), [Marian](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/marian), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MT5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mt5), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [NLLB](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb), [NLLB-MOE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nllb-moe), [Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus), [PEGASUS-X](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/pegasus_x), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/prophetnet), [SwitchTransformers](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/switch_transformers), [T5](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/t5), [XLM-ProphetNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-prophetnet)\n",
    "\n",
    "<!--End of the generated tip-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925d76de",
   "metadata": {},
   "source": [
    "## Load OPUS Books dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fb6f3",
   "metadata": {},
   "source": [
    "Start by loading the English-French subset of the [OPUS Books](https://huggingface.co/datasets/opus_books) dataset from the Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "047d009a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a364b",
   "metadata": {},
   "source": [
    "Split the dataset into a train and test set with the [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1f2eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a85afe8",
   "metadata": {},
   "source": [
    "Then take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4c6d6c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '107379',\n",
       " 'translation': {'en': 'A locomotive, running on the rails laid down the evening before, brought the rails to be laid on the morrow, and advanced upon them as fast as they were put in position.',\n",
       "  'fr': \"Une locomotive, roulant sur les rails de la veille, apportait les rails du lendemain, et courait √† leur surface au fur et √† mesure qu'ils √©taient pos√©s.\"}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299dce92",
   "metadata": {},
   "source": [
    "`translation`: an English and French translation of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8316e3",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aeff72",
   "metadata": {},
   "source": [
    "The next step is to load a T5 tokenizer to process the English-French language pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "df460a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e25e0fe7294aecaa16b2ad908189b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67f8968dc7eb4986b5d995c3c520fc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9361c30641a4e7e867c3dbaa8ec5e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2eb976",
   "metadata": {},
   "source": [
    "The preprocessing function you want to create needs to:\n",
    "\n",
    "1. Prefix the input with a prompt so T5 knows this is a translation task. Some models capable of multiple NLP tasks require prompting for specific tasks.\n",
    "2. Tokenize the input (English) and target (French) separately because you can't tokenize French text with a tokenizer pretrained on an English vocabulary.\n",
    "3. Truncate sequences to be no longer than the maximum length set by the `max_length` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45ca19e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be053a",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "feba4f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031df7a02d4d4fadb4d695918859557b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101668 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1ea29a2f43546c8a76f82ca2115f5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a716ad",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DataCollatorForSeq2Seq](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForSeq2Seq). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd9eee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint) # padding - default is True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c81cbbd",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69fe963",
   "metadata": {},
   "source": [
    "Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) metric (see the Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4fdc250d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install transformers datasets evaluate sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "997f30b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f01c28f0af1499bb8cb2f794d815624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\") # sacrebleu - to calculate Bleu that is a metric used in translations tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8482a3",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the SacreBLEU score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a836ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de515895",
   "metadata": {},
   "source": [
    "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56e1c3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1cc6d7",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "If you aren't familiar with finetuning a model with the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer), take a look at the basic tutorial [here](https://huggingface.co/docs/transformers/main/en/tasks/../training#train-with-pytorch-trainer)!\n",
    "\n",
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! Load T5 with [AutoModelForSeq2SeqLM](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSeq2SeqLM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "407b4ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bbffbda6f84aa7ac3a0c5a0114aecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56ddd9d7dc24682a1577d60c8a6573b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77340c2c455d4651927b2d34dccc457d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4065d567",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [Seq2SeqTrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the SacreBLEU metric and save the training checkpoint.\n",
    "2. Pass the training arguments to [Seq2SeqTrainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "939e0255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12710' max='12710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12710/12710 23:41:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.865900</td>\n",
       "      <td>1.628656</td>\n",
       "      <td>5.591600</td>\n",
       "      <td>17.609500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.807400</td>\n",
       "      <td>1.604612</td>\n",
       "      <td>5.734600</td>\n",
       "      <td>17.605100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "C:\\Users\\migue\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\generation\\utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12710, training_loss=1.8748395582149575, metrics={'train_runtime': 85351.3625, 'train_samples_per_second': 2.382, 'train_steps_per_second': 0.149, 'total_flos': 5006877955325952.0, 'train_loss': 1.8748395582149575, 'epoch': 2.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_opus_books_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    push_to_hub=True,  \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6af3e1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77625ac0592740a5b4207048fc84af4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/miguelactc27/my_awesome_opus_books_model/tree/main/'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2b0be9",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "For a more in-depth example of how to finetune a model for translation, take a look at the corresponding\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)\n",
    "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6eee3a5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac3d507",
   "metadata": {},
   "source": [
    "Great, now that you've finetuned a model, you can use it for inference!\n",
    "\n",
    "Come up with some text you'd like to translate to another language. For T5, you need to prefix your input depending on the task you're working on. For translation from English to French, you should prefix your input as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "40b9e160",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"translate English to French: Legumes share resources with nitrogen-fixing bacteria.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17207fb1",
   "metadata": {},
   "source": [
    "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for translation with your model, and pass your text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7fbd96b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': \"C'√©tait un chef-d'oeuvre, non enti√®rement fid√®le aux livres, mais enthrallant du d√©but √† la fin, peut-√™tre mon pr√©f√©r√© des trois.\"}]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\", model=\"miguelactc27/my_awesome_opus_books_model\")\n",
    "translator(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff103e2",
   "metadata": {},
   "source": [
    "You can also manually replicate the results of the `pipeline` if you'd like:\n",
    "\n",
    "Tokenize the text and return the `input_ids` as PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ef08696a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"miguelactc27/my_awesome_opus_books_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f6cacd",
   "metadata": {},
   "source": [
    "Use the [generate()](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) method to create the translation. For more details about the different text generation strategies and parameters for controlling generation, check out the [Text Generation](https://huggingface.co/docs/transformers/main/en/tasks/../main_classes/text_generation) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b094dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"miguelactc27/my_awesome_opus_books_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4811cf8d",
   "metadata": {},
   "source": [
    "Decode the generated token ids back into text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0c64305e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'masterpiece and was certainly an exceptional masterpiece. Not fully faithful to the books, but enthralling from start to finish. Might be my favorite of the three.'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e30b3",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef8f6c7",
   "metadata": {},
   "source": [
    "Text classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like üôÇ positive, üôÅ negative, or üòê neutral to a sequence of text.\n",
    "\n",
    "![1](https://github.com/huggingface/notebooks/blob/main/examples/images/text_classification.png?raw=1)\n",
    "\n",
    "\n",
    "This guide will show you how to:\n",
    "\n",
    "1. Finetune [DistilBERT](https://huggingface.co/distilbert-base-uncased) on the [IMDb](https://huggingface.co/datasets/imdb) dataset to determine whether a movie review is positive or negative.\n",
    "2. Use your finetuned model for inference.\n",
    "\n",
    "<Tip>\n",
    "The task illustrated in this tutorial is supported by the following model architectures:\n",
    "\n",
    "<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->\n",
    "\n",
    "[ALBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/albert), [BART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bart), [BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bert), [BigBird](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/big_bird), [BigBird-Pegasus](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bigbird_pegasus), [BioGpt](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/biogpt), [BLOOM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/bloom), [CamemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/camembert), [CANINE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/canine), [ConvBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/convbert), [CTRL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ctrl), [Data2VecText](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/data2vec-text), [DeBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta), [DeBERTa-v2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/deberta-v2), [DistilBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/distilbert), [ELECTRA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/electra), [ERNIE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie), [ErnieM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ernie_m), [ESM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/esm), [FlauBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/flaubert), [FNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/fnet), [Funnel Transformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/funnel), [GPT-Sw3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt-sw3), [OpenAI GPT-2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt2), [GPTBigCode](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_bigcode), [GPT Neo](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neo), [GPT NeoX](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gpt_neox), [GPT-J](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/gptj), [I-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/ibert), [LayoutLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlm), [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv2), [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/layoutlmv3), [LED](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/led), [LiLT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/lilt), [LLaMA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/llama), [Longformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/longformer), [LUKE](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/luke), [MarkupLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/markuplm), [mBART](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mbart), [MEGA](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mega), [Megatron-BERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/megatron-bert), [MobileBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mobilebert), [MPNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mpnet), [MVP](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/mvp), [Nezha](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nezha), [Nystr√∂mformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/nystromformer), [OpenLlama](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/open-llama), [OpenAI GPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/openai-gpt), [OPT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/opt), [Perceiver](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/perceiver), [PLBart](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/plbart), [QDQBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/qdqbert), [Reformer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/reformer), [RemBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/rembert), [RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta), [RoBERTa-PreLayerNorm](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roberta-prelayernorm), [RoCBert](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roc_bert), [RoFormer](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/roformer), [SqueezeBERT](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/squeezebert), [TAPAS](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/tapas), [Transformer-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/transfo-xl), [XLM](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm), [XLM-RoBERTa](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta), [XLM-RoBERTa-XL](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlm-roberta-xl), [XLNet](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xlnet), [X-MOD](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/xmod), [YOSO](https://huggingface.co/docs/transformers/main/en/tasks/../model_doc/yoso)\n",
    "\n",
    "\n",
    "\n",
    "<!--End of the generated tip-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c53a4",
   "metadata": {},
   "source": [
    "## Load IMDb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5f37b",
   "metadata": {},
   "source": [
    "Start by loading the IMDb dataset from the Datasets library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "620d2247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c874bfe9c94d3e8e90e326ab46f2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6f589d433f4d2cab96822cd4f8bca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|‚ñà‚ñâ        | 4.19M/21.0M [00:00<00:03, 4.33MB/s]\u001b[A\n",
      "Downloading data:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 12.6M/21.0M [00:01<00:01, 8.02MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 21.0M/21.0M [00:02<00:00, 9.26MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  20%|‚ñà‚ñà        | 4.19M/20.5M [00:00<00:03, 4.43MB/s]\u001b[A\n",
      "Downloading data:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 12.6M/20.5M [00:01<00:01, 7.60MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20.5M/20.5M [00:02<00:00, 8.94MB/s]\u001b[A\n",
      "\n",
      "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  10%|‚ñâ         | 4.19M/42.0M [00:01<00:09, 4.08MB/s]\u001b[A\n",
      "Downloading data:  30%|‚ñà‚ñà‚ñâ       | 12.6M/42.0M [00:01<00:04, 7.32MB/s]\u001b[A\n",
      "Downloading data:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 21.0M/42.0M [00:02<00:02, 8.33MB/s]\u001b[A\n",
      "Downloading data:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 29.4M/42.0M [00:03<00:01, 8.58MB/s]\u001b[A\n",
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42.0M/42.0M [00:04<00:00, 9.24MB/s]\u001b[A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34c570dd27c436f9f39132e56fb6e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77b98b32a26c40b49b4794f1a7ec55ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c10dbdc1a04510be4981f5910ab561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f3a50c1ea504a6aa240fc20f4143295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69cb61",
   "metadata": {},
   "source": [
    "Then take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fb8b9921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clich√©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74fd33",
   "metadata": {},
   "source": [
    "There are two fields in this dataset:\n",
    "\n",
    "- `text`: the movie review text.\n",
    "- `label`: a value that is either `0` for a negative review or `1` for a positive review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e017df",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef645c8",
   "metadata": {},
   "source": [
    "The next step is to load a DistilBERT tokenizer to preprocess the `text` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "312cb54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1b1f5e227141aeabf925e4d71547ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bceb84a4ffa940df8fdbd26a79ae29af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b4d4f136bc433284d509af8d7e31d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf0fa9a8c4c4efc9e4e8e7e2ba1f28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b9bd36",
   "metadata": {},
   "source": [
    "Create a preprocessing function to tokenize `text` and truncate sequences to be no longer than DistilBERT's maximum input length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69e181af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e7dc1",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use Datasets [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1746091c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d534dcffb00466b8aea2c7f46e0dd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab938d9e1af2423f80f023e63c34e01b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba03213b7aa24348b308c41f0ade0820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_imdb = imdb.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2e0d9",
   "metadata": {},
   "source": [
    "Now create a batch of examples using [DataCollatorWithPadding](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding). It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ab6c4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72740519",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3228a8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b5fb98b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f785a8e58e45405c964cc68043502658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f00a62d",
   "metadata": {},
   "source": [
    "Then create a function that passes your predictions and labels to [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) to calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fd911866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dcd4a1",
   "metadata": {},
   "source": [
    "Your `compute_metrics` function is ready to go now, and you'll return to it when you setup your training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6849d3e1",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b772e2",
   "metadata": {},
   "source": [
    "Before you start training your model, create a map of the expected ids to their labels with `id2label` and `label2id`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "69716550",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311a8afb",
   "metadata": {},
   "source": [
    "</Tip>\n",
    "\n",
    "You're ready to start training your model now! Load DistilBERT with [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) along with the number of expected labels, and the label mappings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ddd69132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c87da743dae4562bf850ab1fe854f15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006fbc9",
   "metadata": {},
   "source": [
    "At this point, only three steps remain:\n",
    "\n",
    "1. Define your training hyperparameters in [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments). The only required parameter is `output_dir` which specifies where to save your model. At the end of each epoch, the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) will evaluate the accuracy and save the training checkpoint.\n",
    "2. Pass the training arguments to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) along with the model, dataset, tokenizer, data collator, and `compute_metrics` function.\n",
    "3. Call [train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to finetune your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5b5f4891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3126' max='3126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3126/3126 9:40:58, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.222400</td>\n",
       "      <td>0.269205</td>\n",
       "      <td>0.909120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.231339</td>\n",
       "      <td>0.931440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3126, training_loss=0.20524005011267488, metrics={'train_runtime': 34865.2319, 'train_samples_per_second': 1.434, 'train_steps_per_second': 0.09, 'total_flos': 6564686875195392.0, 'train_loss': 0.20524005011267488, 'epoch': 2.0})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf83699d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/miguelactc27/my_awesome_model/tree/main/'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf844216",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "[Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) applies dynamic padding by default when you pass `tokenizer` to it. In this case, you don't need to specify a data collator explicitly.\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2dafd9",
   "metadata": {},
   "source": [
    "<Tip>\n",
    "\n",
    "For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding\n",
    "[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n",
    "or [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n",
    "\n",
    "</Tip>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1fd0f",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49368a1b",
   "metadata": {},
   "source": [
    "Great, now that you've finetuned a model, you can use it for inference!\n",
    "\n",
    "Grab some text you'd like to run inference on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39802e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce8ba49",
   "metadata": {},
   "source": [
    "The simplest way to try out your finetuned model for inference is to use it in a [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline). Instantiate a `pipeline` for sentiment analysis with your model, and pass your text to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1906dd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a91e62a7a464b99a0ba4cc198d88efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba28cda52b64aa590bda3b97967b77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "657493f56ad24a9493b1c66607a7a5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24597a59601e47dfb5f6733e911fbe08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572232c66ce144f2b893847a41c8a4af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776474a553b64a17ae1ef0abe64b0361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading added_tokens.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a260c67a975042ad87cb0fb4f9622acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9972701668739319}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"miguelactc27/my_awesome_model\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11d8ec6",
   "metadata": {},
   "source": [
    "You can also manually replicate the results of the `pipeline` if you'd like:\n",
    "\n",
    "Tokenize the text and return PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "05417668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"miguelactc27/my_awesome_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72425105",
   "metadata": {},
   "source": [
    "Pass your inputs to the model and return the `logits`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "244cb0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a090ead6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"miguelactc27/my_awesome_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0aeb04",
   "metadata": {},
   "source": [
    "Get the class with the highest probability, and use the model's `id2label` mapping to convert it to a text label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6838fd13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'POSITIVE'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class_id = logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
